{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df17843e-ac7d-4b5f-b983-285fb6f5ecf5",
   "metadata": {},
   "source": [
    "# How to navigate scrips and data in this repository\n",
    "\n",
    "Data is separated into 'raw' and 'clean' and within each of these, 'fine' and 'broad' scale.\n",
    "\n",
    "Code is diveded into the main scripts that conduct the analyses and helper scripts which prepare the data before analyses. The helper scripts are supposed to be run just once, as they are specific for each dataset they handle, as these datasets have different structures. The main scripts are supposed to be general and can be applied to any data that have been tokenized. \n",
    "\n",
    "### Main scritps\n",
    "Script 1: `01_tokenize_texts.py` takes the combined titles and abstracts from the raw data files of each country to make the tokenized data that will be used to create corpuses and dictionaries for the LDA. It uses two supporting files: 1) stop-words.txt and 2) titles-abstracts-directories.txt. Number 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc08f1-9e0e-442d-ab80-f72b7369be1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
